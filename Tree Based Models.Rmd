---
title: "Tree Based Models"
author: "Seth Grace, Jack Julian"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: show
    df_print: paged
    number_sections: yes
    theme: readable
    toc: yes
    toc_float: yes
    code_download: yes
  word_document:
    toc: no
---

```{r setup, include=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE)
knitr::opts_chunk$set(fig.width=8, fig.height=6)

if(require(pacman)==0)
   {install.packages("pacman")}
pacman::p_load(devtools,caret,cluster,dplyr,fastDummies,leaps,pacman,tidyverse,skimr,fastDummies,GGally,DataExplorer,ggrepel,ggthemes,dslabs,scatterplot3d,corrplot, ggforce)

if (!require(mlba)) {
  library(devtools)
  install_github("gedeck/mlba/mlba", force=FALSE)
}
pacman::p_load(mlba,tidyverse)
```


# Read in the data

The following dataset will be used to predict `loan_default`.

```{r}
df=readRDS("M:/MSBA/ISA 591/EDA_cleaned.RDS")
head(df)
```

# Partition the dataset.

Partition the data into a 70/30 training/holdout set.  Use `set.seed(123)` to ensure reproducibility. Print the dimensions of the training and holdout sets.

```{r}
set.seed(123) #always set a random seed immediately before you partition your data
trainIndex <- caret::createDataPartition(df$loan_default, p = 0.7, list = FALSE)
train.df <- df[trainIndex, ]
holdout.df <- df[-trainIndex, ]
dim(train.df)
dim(holdout.df)

```


# Decision Tree
```{r}
library(rpart)
library(rpart.plot)

# Fit decision tree using Entropy
loan.tree <- rpart(
  loan_default ~ .,                 
  data = train.df,         
  method = "class",
  parms = list(split = "information"),
  control = rpart.control(cp = 0.0001)
)

# Plot the tree
rpart.plot(
  loan.tree,
  extra = 1,
  fallen.leaves = FALSE,
  main = "Loan Default Decision Tree (Entropy)"
)

# Cross-validated tree to pick cp
cv_tree <- rpart(
  loan_default ~ .,
  data = train.df,
  method = "class",
  parms = list(split = "information"),
  control = rpart.control(cp = 0.001) # small cp to allow full growth first
)

# Select best cp
best_cp <- cv_tree$cptable[which.min(cv_tree$cptable[,"xerror"]), "CP"]
best_cp

# Prune model using best cp
pruned.loan.tree <- prune(cv_tree, cp = best_cp)

# Plot pruned tree
rpart.plot(
  pruned.loan.tree,
  extra = 1,
  fallen.leaves = FALSE,
  main = "Pruned Decision Tree â€” Loan Default"
)

```

# RF Model
```{r}
cores=parallel::detectCores()
cores

# Here we are using cores-1, but you can adjust this based on your machine
cl <- parallel::makeCluster(cores-1)  # Set CPU cores for parallel execution
registerDoParallel(cl)  # Register parallel backend


# Set up trainControl
cv_control <- trainControl(method = "cv", number = 5)

# Define a grid of `mtry` values to search over
mtry_grid <- expand.grid(mtry = seq(1, ncol(train.df) - 1, by = 1))  

# Train the Random Forest model with cross-validation
rf_cv <- train(loan_default ~ ., 
                  data = train.df,
                  method = "rf",  # Uses randomForest under the hood
                  trControl = cv_control,  # Apply 5-fold CV
                  tuneGrid = mtry_grid,  # Search over `mtry`
                  ntree = 50,  # Fixed number of trees
                  importance = TRUE,
                  nodesize = 5)


rf_cv
plot(rf_cv)
varImp(rf_cv)

# Step 4: Stop Parallel Processing ----
stopCluster(cl)  # Shut down parallel cluster
registerDoSEQ()  # Reset to sequential processing
```



# XG Boost
```{r}
# Here we are using cores-1, but you can adjust this based on your machine
cl <- parallel::makeCluster(cores-1)  # Set CPU cores for parallel execution
registerDoParallel(cl)  # Register parallel backend


library(caret)
set.seed(1)

# CV settings (same as RF)
cv_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# XGBoost model (baseline)
xgb_base <- train(
  loan_default ~ ., 
  data = train.df,
  method = "xgbTree",
  trControl = cv_control,
  tuneLength = 5,      # just like class notes
  metric = "ROC"
)

xgb_base
varImp(xgb_base)
plot(xgb_base)

# Step 4: Stop Parallel Processing ----
stopCluster(cl)  # Shut down parallel cluster
registerDoSEQ()  # Reset to sequential processing
```


# Evaluate Models

## Random Forest
```{r}
# Predicted class labels
rf.pred.class <- predict(rf_cv, newdata = holdout.df)

# Predicted probabilities for ROC later if needed
rf.pred.prob <- predict(rf_cv, newdata = holdout.df, type = "prob")[, "Yes"]

# Confusion Matrix
confusionMatrix(
  rf.pred.class,
  holdout.df$loan_default,
  positive = "Yes"
)
```

## XG Boost
```{r}
xgb.pred.class <- predict(xgb_base, newdata = holdout.df)
confusionMatrix(
  xgb.pred.class,
  holdout.df$loan_default,
  positive = "Yes"
)
```


## Decision Tree
```{r}
tree.pred.class <- predict(pruned.loan.tree, newdata = holdout.df, type = "class")
confusionMatrix(
  tree.pred.class,
  holdout.df$loan_default,
  positive = "Yes"
)
```

