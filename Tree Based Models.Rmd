---
title: "Tree Based Models"
author: "Seth Grace, Jack Julian"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: show
    df_print: paged
    number_sections: yes
    theme: readable
    toc: yes
    toc_float: yes
    code_download: yes
  word_document:
    toc: no
---

```{r setup, include=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,warning=FALSE)
knitr::opts_chunk$set(fig.width=8, fig.height=6)

if(require(pacman)==0)
   {install.packages("pacman")}
pacman::p_load(devtools,caret,cluster,dplyr,fastDummies,leaps,pacman,tidyverse,skimr,fastDummies,GGally,DataExplorer,ggrepel,ggthemes,dslabs,scatterplot3d,corrplot, ggforce, doParallel)

if (!require(mlba)) {
  library(devtools)
  install_github("gedeck/mlba/mlba", force=FALSE)
}
pacman::p_load(mlba,tidyverse)
```


# Read in the data

The following dataset will be used to predict `loan_default`.

```{r}
df=readRDS("M:/MSBA/ISA 591/EDA_cleaned.RDS")
head(df)
```

# Partition the dataset.

Partition the data into a 70/30 training/holdout set.  Use `set.seed(123)` to ensure reproducibility. Print the dimensions of the training and holdout sets.

```{r}
# --- Downsample the majority class in the training set ---
library(caret)

set.seed(123)
train.down <- downSample(
  x = df[, setdiff(names(df), "loan_default")],  # predictors only
  y = df$loan_default,                                 # target variable
  yname = "loan_default"                                     # name for the new target column
)

# target smaller sample size, e.g., 5,000 rows total
set.seed(123)
train.smaller <- df %>%
  dplyr::sample_n(25000)

table(train.smaller$loan_default)

# check class distribution
table(train.down$loan_default)

```
```{r}
#loan.logit <- glm(
 # loan_default ~ ., 
  ##amily = binomial


#summary(loan.logit)
```


# Decision Tree
```{r}
library(rpart)
library(rpart.plot)

# Fit decision tree using Entropy
loan.tree <- rpart(
  loan_default ~ .,                 
  data = train.smaller,         
  method = "class",
  parms = list(split = "information"),
  control = rpart.control(cp = 0.0001)
)

# Plot the tree
rpart.plot(
  loan.tree,
  extra = 1,
  fallen.leaves = FALSE,
  main = "Loan Default Decision Tree (Entropy)"
)

# Cross-validated tree to pick cp
cv_tree <- rpart(
  loan_default ~ .,
  data = train.smaller,
  method = "class",
  parms = list(split = "information"),
  control = rpart.control(cp = 0.001) # small cp to allow full growth first
)

# Select best cp
best_cp <- cv_tree$cptable[which.min(cv_tree$cptable[,"xerror"]), "CP"]
best_cp

# Prune model using best cp
pruned.loan.tree <- prune(cv_tree, cp = best_cp)

# Plot pruned tree
rpart.plot(
  pruned.loan.tree,
  extra = 1,
  fallen.leaves = FALSE,
  main = "Pruned Decision Tree — Loan Default"
)

```

# RF Model
```{r}
cores=parallel::detectCores()
cores

# Here we are using cores-1, but you can adjust this based on your machine
cl <- parallel::makeCluster(cores-1)  # Set CPU cores for parallel execution
registerDoParallel(cl)  # Register parallel backend


rf_grid <- expand.grid(mtry = seq(1, ncol(train.smaller)-1, by = 2))

cv_control <- trainControl(
  method = "cv", 
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,   # enables AUC scoring
  savePredictions = TRUE
)

rf_cv <- train(
  loan_default ~ .,
  data = train.smaller,
  method = "rf",
  metric = "ROC",           # now optimizes AUC
  trControl = cv_control,
  tuneGrid = rf_grid,
  ntree = 500,
  importance = TRUE,
  nodesize = 3              # smaller nodes → more granular splits
)



rf_cv
plot(rf_cv)
varImp(rf_cv)

# Step 4: Stop Parallel Processing ----
stopCluster(cl)  # Shut down parallel cluster
registerDoSEQ()  # Reset to sequential processing
```



# XG Boost
```{r}
# Here we are using cores-1, but you can adjust this based on your machine
cl <- parallel::makeCluster(cores-1)  # Set CPU cores for parallel execution
registerDoParallel(cl)  # Register parallel backend


library(caret)
set.seed(1)

# CV settings (same as RF)
cv_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary
)

# XGBoost model (baseline)
xgb_base <- train(
  loan_default ~ ., 
  data = train.smaller,
  method = "xgbTree",
  trControl = cv_control,
  tuneLength = 5,      # just like class notes
  metric = "ROC"
)

xgb_base
varImp(xgb_base)
plot(xgb_base)

# Step 4: Stop Parallel Processing ----
stopCluster(cl)  # Shut down parallel cluster
registerDoSEQ()  # Reset to sequential processing
```


# Evaluate Models


## Holdout Data
```{r}
holdout.df <- readRDS("M:/MSBA/ISA 591/Holdout_cleaned.RDS")
```


## Random Forest
```{r}
# Predicted class labels
rf.pred.class <- predict(rf_cv, newdata = holdout.df)

# Predicted probabilities for ROC later if needed
rf.pred.prob <- predict(rf_cv, newdata = holdout.df, type = "prob")[, "Yes"]

# Confusion Matrix
confusionMatrix(
  rf.pred.class,
  holdout.df$loan_default,
  positive = "Yes"
)

# --- Compute and print AUC ---
library(pROC)

roc_obj <- roc(holdout.df$loan_default, rf.pred.prob, levels = c("No", "Yes"))
auc_value <- auc(roc_obj)
print(paste("AUC:", round(auc_value, 4)))
```

## XG Boost
```{r}
# Predicted class labels
xgb.pred.class <- predict(xgb_base, newdata = holdout.df)

# Predicted probabilities (for ROC / AUC)
xgb.pred.prob <- predict(xgb_base, newdata = holdout.df, type = "prob")[, "Yes"]

# Confusion Matrix
confusionMatrix(
  xgb.pred.class,
  holdout.df$loan_default,
  positive = "Yes"
)

# --- Compute and print AUC ---
library(pROC)

roc_obj <- roc(holdout.df$loan_default, xgb.pred.prob, levels = c("No", "Yes"))
auc_value <- auc(roc_obj)
print(paste("AUC:", round(auc_value, 4)))

```


## Decision Tree
```{r}
# Predicted class labels
tree.pred.class <- predict(pruned.loan.tree, newdata = holdout.df, type = "class")

# Confusion Matrix
confusionMatrix(
  tree.pred.class,
  holdout.df$loan_default,
  positive = "Yes"
)

# --- Compute and print AUC ---
# Get predicted probabilities instead of classes
tree.pred.prob <- predict(pruned.loan.tree, newdata = holdout.df, type = "prob")[, "Yes"]

library(pROC)
roc_obj <- roc(holdout.df$loan_default, tree.pred.prob, levels = c("No", "Yes"))
auc_value <- auc(roc_obj)
print(paste("AUC:", round(auc_value, 4)))

```

